{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8db53b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import math\n",
    "import h5py\n",
    "\n",
    "from line_junction_utils import *\n",
    "from robust_line_based_estimator.line_matcher import LineMatcher\n",
    "from robust_line_based_estimator.vp_matcher import vp_matching\n",
    "from robust_line_based_estimator.visualization import plot_images, plot_lines, plot_color_line_matches, plot_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85ccbbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Parameters\n",
    "user = \"ir0\" # or \"daniel\"\n",
    "if user == \"daniel\":\n",
    "    data_path = \"/media/hdd3tb/datasets/\"\n",
    "elif user == \"ir0\":\n",
    "    data_path = \"/home/ir0/data/line_estimator/data/\"\n",
    "else:\n",
    "    print(\"Unknown user!\")\n",
    "\n",
    "dataset_path = data_path + \"scannet/scannet_test_images\"\n",
    "depth_path = data_path + \"scannet/scannet_test_images\"\n",
    "pair_list = data_path + \"scannet/scannet_pairs.txt\"\n",
    "output_path = \".\"\n",
    "\n",
    "matcher_type = \"sold2\"\n",
    "threshold = 10 # Threshold in pixels\n",
    "planar_threshold = 0.005 # 5 cm tolerance\n",
    "visualize = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfc86ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the line matcher\n",
    "if matcher_type == \"lbd\":\n",
    "    # LSD+LBD matcher\n",
    "    matcher = LineMatcher(line_detector='lsd', line_matcher='lbd')\n",
    "else:\n",
    "    # SOLD2 matcher\n",
    "    conf = {\n",
    "        'sold2': {\n",
    "            'ckpt_path': '../third_party/SOLD2/pretrained_models/sold2_wireframe.tar',\n",
    "            'device': 'cpu'\n",
    "        }\n",
    "    }\n",
    "    matcher = LineMatcher(line_detector='sold2', line_matcher='sold2', conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6375e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib widget\n",
    "#\n",
    "#import itertools\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "#import matplotlib.pyplot as plt\n",
    "#\n",
    "#def visualize3d(points):\n",
    "#    fig = plt.figure()\n",
    "#    ax = Axes3D(fig, azim=-85, elev=-77)\n",
    "#    colors = points[:,3:] / 255.0 #np.repeat(points[:,3,np.newaxis], 3, axis=1) / 255.0\n",
    "#    plot = ax.scatter(points[:,0], points[:,1], points[:,2], c=colors)\n",
    "#\n",
    "#    # displaying the plot\n",
    "#    plt.show()\n",
    "#    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d0fd0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def bilinear_interpolation(x, y, depth_img):\n",
    "    '''Interpolate (x,y) from values associated with four points.\n",
    "    The four points are a list of four triplets:  (x, y, value).\n",
    "    The four points can be in any order.  They should form a rectangle.\n",
    "    '''\n",
    "    # See formula at:  http://en.wikipedia.org/wiki/Bilinear_interpolation\n",
    "\n",
    "    points = [(math.floor(x), math.floor(y), 0),\n",
    "        (math.floor(x) + 1, math.floor(y), 0),\n",
    "        (math.floor(x) + 1, math.floor(y) + 1, 0),\n",
    "        (math.floor(x), math.floor(y) + 1, 0)]\n",
    "\n",
    "    try:\n",
    "        for i in range(4):\n",
    "            points[i] = (points[i][0], points[i][1], depth_img[points[i][0], points[i][1]])\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    points = sorted(points)               # order points by x, then by y\n",
    "    (x1, y1, q11), (_x1, y2, q12), (x2, _y1, q21), (_x2, _y2, q22) = points\n",
    "\n",
    "    if x1 != _x1 or x2 != _x2 or y1 != _y1 or y2 != _y2:\n",
    "        raise ValueError('points do not form a rectangle')\n",
    "    if not x1 <= x <= x2 or not y1 <= y <= y2:\n",
    "        raise ValueError('(x, y) not within the rectangle')\n",
    "\n",
    "    return (q11 * (x2 - x) * (y2 - y) +\n",
    "            q21 * (x - x1) * (y2 - y) +\n",
    "            q12 * (x2 - x) * (y - y1) +\n",
    "            q22 * (x - x1) * (y - y1)\n",
    "           ) / ((x2 - x1) * (y2 - y1) + 0.0)\n",
    "\n",
    "def depth_consistency_check(m_lines1, m_lines2, K1, K2, img1, img2, depth_img1, depth_img2, threshold):\n",
    "    # The number of line correspondences\n",
    "    num_matches = m_lines1.shape[0]\n",
    "    valid_line_pairs = []\n",
    "\n",
    "    C1x = K1[0, 2]\n",
    "    C1y = K1[1, 2]\n",
    "    f1x = K1[0, 0]\n",
    "    f1y = K1[1, 1]\n",
    "    C2x = K2[0, 2]\n",
    "    C2y = K2[1, 2]\n",
    "    f2x = K2[0, 0]\n",
    "    f2y = K2[1, 1]\n",
    "\n",
    "    # Calculating the resize ratios for the feature points to the depth maps\n",
    "    ratios1 = tuple(float(ele1) / float(ele2) for ele1, ele2 in zip(depth_img1.shape, img1.shape))\n",
    "    ratios2 = tuple(float(ele1) / float(ele2) for ele1, ele2 in zip(depth_img2.shape, img2.shape))\n",
    "\n",
    "    # Iterate through all line correspondences and check if they are on the same plane\n",
    "    for line_idx1 in range(0, num_matches):\n",
    "        # Depth at the endpoints in the source image\n",
    "        xs11 = m_lines1[line_idx1][0][0] * ratios1[0]\n",
    "        ys11 = m_lines1[line_idx1][0][1] * ratios1[1]\n",
    "        xe11 = m_lines1[line_idx1][1][0] * ratios1[0]\n",
    "        ye11 = m_lines1[line_idx1][1][1] * ratios1[1]\n",
    "        depth11s = bilinear_interpolation(xs11, ys11, depth_img1) / 1000.0\n",
    "        depth11e = bilinear_interpolation(xe11, ye11, depth_img1) / 1000.0\n",
    "\n",
    "        nxs11 = (m_lines1[line_idx1][0][1] - C1x) / f1x\n",
    "        nys11 = (m_lines1[line_idx1][0][0] - C1y) / f1y\n",
    "        nxe11 = (m_lines1[line_idx1][1][1] - C1x) / f1x\n",
    "        nye11 = (m_lines1[line_idx1][1][0] - C1y) / f1y\n",
    "        tangent11 = np.array([nxe11, nye11, depth11e]) - np.array([nxs11, nys11, depth11s])\n",
    "        \n",
    "        if depth11s < 1e-9 or depth11e < 1e-9:\n",
    "            continue\n",
    "\n",
    "        # Depth at the endpoints in the destination image\n",
    "        xs21 = m_lines2[line_idx1][0][0] * ratios2[0]\n",
    "        ys21 = m_lines2[line_idx1][0][1] * ratios2[1]\n",
    "        xe21 = m_lines2[line_idx1][1][0] * ratios2[0]\n",
    "        ye21 = m_lines2[line_idx1][1][1] * ratios2[1]\n",
    "        depth21s = bilinear_interpolation(xs21, ys21, depth_img2) / 1000.0\n",
    "        depth21e = bilinear_interpolation(xe21, ye21, depth_img2) / 1000.0\n",
    "        \n",
    "        nxs21 = (m_lines2[line_idx1][0][1] - C2x) / f2x\n",
    "        nys21 = (m_lines2[line_idx1][0][0] - C2y) / f2y\n",
    "        nxe21 = (m_lines2[line_idx1][1][1] - C2x) / f2x\n",
    "        nye21 = (m_lines2[line_idx1][1][0] - C2y) / f2y\n",
    "        tangent21 = np.array([nxe21, nye21, depth21e]) - np.array([nxs21, nys21, depth21s])\n",
    "\n",
    "        if depth21s < 1e-9 or depth21e < 1e-9:\n",
    "            continue\n",
    "\n",
    "        for line_idx2 in range(line_idx1 + 1, num_matches):\n",
    "            # Depth at the endpoints in the source image\n",
    "            xs12 = m_lines1[line_idx2][0][0] * ratios1[0]\n",
    "            ys12 = m_lines1[line_idx2][0][1] * ratios1[1]\n",
    "            xe12 = m_lines1[line_idx2][1][0] * ratios1[0]\n",
    "            ye12 = m_lines1[line_idx2][1][1] * ratios1[1]\n",
    "            depth12s = bilinear_interpolation(xs12, ys12, depth_img1) / 1000.0\n",
    "            depth12e = bilinear_interpolation(xe12, ye12, depth_img1) / 1000.0\n",
    "\n",
    "            nxs12 = (m_lines1[line_idx2][0][1] - C1x) / f1x\n",
    "            nys12 = (m_lines1[line_idx2][0][0] - C1y) / f1y\n",
    "            nxe12 = (m_lines1[line_idx2][1][1] - C1x) / f1x\n",
    "            nye12 = (m_lines1[line_idx2][1][0] - C1y) / f1y\n",
    "            tangent12 = np.array([nxs12, nys12, depth12s]) - np.array([nxs11, nys11, depth11s])\n",
    "\n",
    "            if depth12s < 1e-9 or depth12e < 1e-9:\n",
    "                continue\n",
    "            \n",
    "            # Depth at the endpoints in the destination image\n",
    "            xs22 = m_lines2[line_idx2][0][0] * ratios2[0]\n",
    "            ys22 = m_lines2[line_idx2][0][1] * ratios2[1]\n",
    "            xe22 = m_lines2[line_idx2][1][0] * ratios2[0]\n",
    "            ye22 = m_lines2[line_idx2][1][1] * ratios2[1]\n",
    "            depth22s = bilinear_interpolation(xs22, ys22, depth_img2) / 1000.0\n",
    "            depth22e = bilinear_interpolation(xe22, ye22, depth_img2) / 1000.0\n",
    "\n",
    "            nxs22 = (m_lines2[line_idx2][0][1] - C2x) / f2x\n",
    "            nys22 = (m_lines2[line_idx2][0][0] - C2y) / f2y\n",
    "            nxe22 = (m_lines2[line_idx2][1][1] - C2x) / f2x\n",
    "            nye22 = (m_lines2[line_idx2][1][0] - C2y) / f2y\n",
    "            tangent22 = np.array([nxs22, nys22, depth22s]) - np.array([nxs21, nys21, depth21s])\n",
    "\n",
    "            if depth22s < 1e-9 or depth22e < 1e-9:\n",
    "                continue\n",
    "            \n",
    "            # Estimate the plane normals\n",
    "            normal1 = np.cross(tangent11, tangent12)\n",
    "            if np.linalg.norm(normal1) < 1e-9:\n",
    "                continue\n",
    "            normal1 /= np.linalg.norm(normal1)\n",
    "            normal2 = np.cross(tangent21, tangent22)\n",
    "            if np.linalg.norm(normal2) < 1e-9:\n",
    "                continue\n",
    "            normal2 /= np.linalg.norm(normal2)\n",
    "\n",
    "            # Calculate the plane offset w.r.t. the origin\n",
    "            d1 = -normal1[0] * nxs11 - normal1[1] * nys11 - normal1[2] * depth11s\n",
    "            d2 = -normal2[0] * nxs21 - normal2[1] * nys21 - normal2[2] * depth21s\n",
    "\n",
    "            # Check the point-to-plane distance of the 4th points\n",
    "            distance1 = abs(normal1[0] * nxe12 + normal1[1] * nye12 + normal1[2] * depth12e + d1)\n",
    "            distance2 = abs(normal2[0] * nxe22 + normal2[1] * nye22 + normal2[2] * depth22e + d2)\n",
    "\n",
    "            #print(distance1, distance2, threshold)\n",
    "            if max(distance1, distance2) < threshold:\n",
    "                valid_line_pairs.append((line_idx1, line_idx2, max(distance1, distance2)))\n",
    "    return valid_line_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8aa8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processingScene(dataset_path, depth_path, source_img_name, dest_img_name, K1, K2, P, matcher, linefile_path, matchfile_path, threshold, planar_threshold, visualize = False):\n",
    "    cnt = 0\n",
    "\n",
    "    img0 = cv2.imread(f\"{dataset_path}/{source_img_name}.jpg\", 0)\n",
    "    img1 = cv2.imread(f\"{dataset_path}/{dest_img_name}.jpg\", 0)\n",
    "\n",
    "    depth_img0 = cv2.imread(f\"{dataset_path}/{source_img_name}.depth.png\", 0)\n",
    "    depth_img1 = cv2.imread(f\"{dataset_path}/{dest_img_name}.depth.png\", 0)\n",
    "    print(f\"{dataset_path}/{source_img_name}.depth.png\")\n",
    "    \n",
    "    import os\n",
    "    print(os.path.exists(f\"{dataset_path}/{source_img_name}.depth.png\"))\n",
    "\n",
    "    ## Detecting line segments in the first image\n",
    "    features0_segments = read_h5(f\"img-{source_img_name}-line-segments\", linefile_path)\n",
    "    features0_descs = read_h5(f\"img-{source_img_name}-descriptor\", linefile_path)\n",
    "    features0_ms_lines = read_h5(f\"img-{source_img_name}-ms_lines\", linefile_path)\n",
    "    features0_scale_factor = read_h5(f\"img-{source_img_name}-scale\", linefile_path)\n",
    "    if features0_segments is None:\n",
    "        features0 = matcher.detect_and_describe_lines(img0)\n",
    "        if matcher.matcher == \"lbd\":\n",
    "            append_h5({f\"img-{source_img_name}-line-segments\": features0[\"line_segments\"],\n",
    "                f\"img-{source_img_name}-descriptor\": features0[\"descriptor\"], \n",
    "                f\"img-{source_img_name}-ms_lines\": convert_ms_lines(features0[\"ms_lines\"])}, \n",
    "                linefile_path)\n",
    "        elif matcher.matcher == \"sold2\":\n",
    "            append_h5({f\"img-{source_img_name}-line-segments\": features0[\"line_segments\"],\n",
    "                f\"img-{source_img_name}-descriptor\": features0[\"descriptor\"],\n",
    "                f\"img-{source_img_name}-scale\": features0[\"scale_factor\"] }, \n",
    "                linefile_path)\n",
    "    else:\n",
    "        if matcher.matcher == \"lbd\":\n",
    "            features0 = { \"line_segments\": features0_segments, \n",
    "                \"descriptor\": features0_descs,\n",
    "                \"ms_lines\": parse_ms_lines(features0_ms_lines) }\n",
    "        elif matcher.matcher == \"sold2\":\n",
    "            features0 = { \"line_segments\": torch.tensor(features0_segments), \n",
    "                \"descriptor\": torch.tensor(features0_descs), \n",
    "                \"scale_factor\": torch.tensor(features0_scale_factor) }\n",
    "        \n",
    "    # Detecting line segments in the second image\n",
    "    features1_segments = read_h5(f\"img-{dest_img_name}-line-segments\", linefile_path)\n",
    "    features1_descs = read_h5(f\"img-{dest_img_name}-descriptor\", linefile_path)\n",
    "    features1_ms_lines = read_h5(f\"img-{dest_img_name}-ms_lines\", linefile_path)\n",
    "    features1_scale_factor = read_h5(f\"img-{dest_img_name}-scale\", linefile_path)\n",
    "    if features1_segments is None:\n",
    "        features1 = matcher.detect_and_describe_lines(img1)\n",
    "        if matcher.matcher == \"lbd\":\n",
    "            append_h5({f\"img-{dest_img_name}-line-segments\": features1[\"line_segments\"],\n",
    "                f\"img-{dest_img_name}-descriptor\": features1[\"descriptor\"], \n",
    "                f\"img-{dest_img_name}-ms_lines\": convert_ms_lines(features1[\"ms_lines\"])}, \n",
    "                linefile_path)\n",
    "        elif matcher.matcher == \"sold2\":\n",
    "            append_h5({f\"img-{dest_img_name}-line-segments\": features1[\"line_segments\"],\n",
    "                f\"img-{dest_img_name}-descriptor\": features1[\"descriptor\"],\n",
    "                f\"img-{dest_img_name}-scale\": features1[\"scale_factor\"] }, \n",
    "                linefile_path)\n",
    "    else:\n",
    "        if matcher.matcher == \"lbd\":\n",
    "            features1 = { \"line_segments\": features1_segments, \n",
    "                \"descriptor\": features1_descs,\n",
    "                \"ms_lines\": parse_ms_lines(features1_ms_lines) }\n",
    "        elif matcher.matcher == \"sold2\":\n",
    "            features1 = { \"line_segments\": torch.tensor(features1_segments), \n",
    "                \"descriptor\": torch.tensor(features1_descs), \n",
    "                \"scale_factor\": torch.tensor(features1_scale_factor) }\n",
    "\n",
    "    ## Matching\n",
    "    m_lines0 = read_h5(f\"{source_img_name}-{dest_img_name}-m_lines0\", matchfile_path)\n",
    "    m_lines1 = read_h5(f\"{source_img_name}-{dest_img_name}-m_lines1\", matchfile_path)\n",
    "    if m_lines0 is None:\n",
    "        _, m_lines0, m_lines1 = matcher.match_lines(img0, img1, features0, features1)\n",
    "        append_h5({f\"{source_img_name}-{dest_img_name}-m_lines0\": m_lines0,\n",
    "            f\"{source_img_name}-{dest_img_name}-m_lines1\": m_lines1}, \n",
    "            matchfile_path)\n",
    "\n",
    "    # Get the line pairs that are co-planar\n",
    "    print(depth_img0, depth_img1)\n",
    "    valid_line_pairs = depth_consistency_check(m_lines0, m_lines1, K1, K2, img0, img1, depth_img0, depth_img1, planar_threshold)\n",
    "    print(f\"{len(valid_line_pairs)}/{len(m_lines0) * (len(m_lines0) - 1) / 2} line pairs survived co-planarity check.\")\n",
    "\n",
    "    # Get all possible line junctions\n",
    "    keypoints1, keypoints2 = get_line_junctions(m_lines0, m_lines1, valid_line_pairs)\n",
    "\n",
    "    # Normalize the obtained keypoints by the camera matrix\n",
    "    if keypoints1.shape[0] > 0:\n",
    "        kp1n = normalize_keypoints(keypoints1[:, [1, 0]], K1).astype(np.float64)\n",
    "        kp2n = normalize_keypoints(keypoints2[:, [1, 0]], K2).astype(np.float64)\n",
    "\n",
    "    # Normalizing the threshold by the camera matrices\n",
    "    normalized_threshold = threshold / (0.25 * (K1[0, 0] + K1[1, 1] + K2[0, 0] + K2[1, 1]))\n",
    "\n",
    "    # Composing the projection matrices from the rotations and translations\n",
    "    P1 = np.concatenate([np.identity(3), np.zeros((3, 1))], axis=1)\n",
    "    P2 = P\n",
    "\n",
    "    # Checking each point's consistency with the camera poses by\n",
    "    # triangulating them and, then, calculating the re-projection error.\n",
    "    # for point_idx in range(kp1n.shape[0]):\n",
    "    if keypoints1.shape[0] > 0:\n",
    "        #s_errors = sampson_errors(keypoints1, keypoints2, K1 @ P1, K2 @ P2)\n",
    "        points3d, status = polynomial_triangulation(kp1n, P1, kp2n, P2)\n",
    "        errors = reprojection_errors(keypoints1[:, [1, 0]], keypoints2[:, [1, 0]], points3d, K1 @ P1, K2 @ P2)\n",
    "        inlier_mask = errors < threshold  \n",
    "\n",
    "    if visualize:\n",
    "        if keypoints1.shape[0] > 0:\n",
    "            print(f\"Point number = {kp1n.shape[0]}\")   \n",
    "            print(f\"Inlier number = {np.sum(inlier_mask)}\") \n",
    "\n",
    "            # Select points that fall inside the images\n",
    "            points_inside1 = (keypoints1[:,1] >= 0) & (keypoints1[:,1] < img0.shape[1]) & (keypoints1[:,0] >= 0) & (keypoints1[:,0] < img0.shape[0])\n",
    "            points_inside2 = (keypoints2[:,1] >= 0) & (keypoints2[:,1] < img1.shape[1]) & (keypoints2[:,0] >= 0) & (keypoints2[:,0] < img1.shape[0])\n",
    "            points_inside = points_inside1 & points_inside2 & inlier_mask\n",
    "\n",
    "        # Plot the matches\n",
    "        plot_images([img0, img1], ['Keypoints 0', 'Keypoints 1'])\n",
    "        plot_color_line_matches([m_lines0[:, :, [1, 0]], m_lines1[:, :, [1, 0]]])\n",
    "\n",
    "        if keypoints1.shape[0] > 0:\n",
    "            tmpKps1 = np.stack((keypoints1[points_inside, 1], keypoints1[points_inside, 0]), axis=1)\n",
    "            tmpKps2 = np.stack((keypoints2[points_inside, 1], keypoints2[points_inside, 0]), axis=1)\n",
    "            plot_matches(tmpKps1, tmpKps2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2d2220",
   "metadata": {},
   "source": [
    "## Line detection and description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "441231d4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ir0/data/line_estimator/data/scannet/scannet_test_images/scene0707_00_frame-000015.depth.png\n",
      "False\n",
      "None None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@187.658] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('/home/ir0/data/line_estimator/data/scannet/scannet_test_images/scene0707_00_frame-000015.depth.png'): can't open/read file: check file path/integrity\n",
      "[ WARN:0@187.658] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('/home/ir0/data/line_estimator/data/scannet/scannet_test_images/scene0707_00_frame-000585.depth.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m P \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mresize(np\u001b[38;5;241m.\u001b[39marray(items[\u001b[38;5;241m22\u001b[39m:])\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64), (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m4\u001b[39m))[:\u001b[38;5;241m3\u001b[39m,:]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Iterating through the image pairs in the scene\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mprocessingScene\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_img_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdest_img_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mK2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatcher\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/lines_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmatcher_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_scannet.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/matches_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmatcher_type\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_scannet.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplanar_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpair_number\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m pair_number \u001b[38;5;241m>\u001b[39m visualize:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [23], line 79\u001b[0m, in \u001b[0;36mprocessingScene\u001b[0;34m(dataset_path, depth_path, source_img_name, dest_img_name, K1, K2, P, matcher, linefile_path, matchfile_path, threshold, planar_threshold, visualize)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Get the line pairs that are co-planar\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(depth_img0, depth_img1)\n\u001b[0;32m---> 79\u001b[0m valid_line_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mdepth_consistency_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_lines0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm_lines1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_img0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_img1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplanar_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(valid_line_pairs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(m_lines0) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mlen\u001b[39m(m_lines0) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m line pairs survived co-planarity check.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Get all possible line junctions\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [5], line 51\u001b[0m, in \u001b[0;36mdepth_consistency_check\u001b[0;34m(m_lines1, m_lines2, K1, K2, img1, img2, depth_img1, depth_img2, threshold)\u001b[0m\n\u001b[1;32m     48\u001b[0m f2y \u001b[38;5;241m=\u001b[39m K2[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Calculating the resize ratios for the feature points to the depth maps\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m ratios1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(ele1) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ele2) \u001b[38;5;28;01mfor\u001b[39;00m ele1, ele2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mdepth_img1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m, img1\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m     52\u001b[0m ratios2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(ele1) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(ele2) \u001b[38;5;28;01mfor\u001b[39;00m ele1, ele2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(depth_img2\u001b[38;5;241m.\u001b[39mshape, img2\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Iterate through all line correspondences and check if they are on the same plane\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "with open(pair_list, \"r\") as file:\n",
    "    pair_number = 0\n",
    "    while True:\n",
    "        # Get next line from file\n",
    "        line = file.readline()\n",
    "\n",
    "        # If line is empty\n",
    "        # end of file is reached\n",
    "        if not line:\n",
    "            break\n",
    "\n",
    "        items = line.split(\" \")\n",
    "        pair_number += 1\n",
    "\n",
    "        source_img_name = items[0][11 : 11 + 12] + \"_\" + items[0][29 : 29 + 12]\n",
    "        dest_img_name = items[1][11 : 11 + 12] + \"_\" + items[1][29 : 29 + 12]\n",
    "        K1 = np.resize(np.array(items[4:13]).astype(np.float64), (3, 3))\n",
    "        K2 = np.resize(np.array(items[13:22]).astype(np.float64), (3, 3))\n",
    "        P = np.resize(np.array(items[22:]).astype(np.float64), (4, 4))[:3,:]\n",
    "\n",
    "        # Iterating through the image pairs in the scene\n",
    "        processingScene(\n",
    "            dataset_path,\n",
    "            depth_path,\n",
    "            source_img_name, \n",
    "            dest_img_name, \n",
    "            K1,\n",
    "            K2, \n",
    "            P, \n",
    "            matcher,\n",
    "            f\"{output_path}/lines_{matcher_type}_scannet.h5\", \n",
    "            f\"{output_path}/matches_{matcher_type}_scannet.h5\", \n",
    "            threshold,\n",
    "            planar_threshold,\n",
    "            visualize = pair_number <= visualize)\n",
    "\n",
    "        if visualize > 0 and pair_number > visualize:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f2689",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2328b2900916ebe49871cf7fb418aafb5182e08877aeb0eb3e6317399373126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
